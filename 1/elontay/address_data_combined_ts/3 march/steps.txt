Data Loading and Splitting:

Loaded the dataset (all columns) from a CSV file.
Dropped the non-predictive identifier column (“Address”) and separated the target variable (“FLAG”).
Split the data into training and testing sets using stratification to maintain class proportions.

Preprocessing – Raw Features Transformation:

Identified 12 raw feature columns (e.g., transaction timings, counts, Ether amounts).
Applied a log transformation to these raw features using a conditional lambda: if a value is >0, use np.log(x); otherwise, set it to 0. This reduces skewness in positive-valued features and handles non-positive values gracefully.

Preprocessing – Scaling:

Used MinMaxScaler to scale all features (both the transformed raw features and the derived features) to a consistent range.

Model Training with Hyperparameter Tuning:

Initialized an XGBoost classifier with the binary logistic objective.
Set up a parameter grid (varying max_depth, n_estimators, and learning_rate) and used GridSearchCV with 5-fold cross-validation, optimizing for the F1 score.
Trained the model on the preprocessed training data and selected the best hyperparameter combination based on cross-validation performance.

Pipeline Construction and Saving:

Built a scikit-learn Pipeline that encapsulates the log transformation (via a custom transformer), scaling, and the trained XGBoost model.
Saved the entire pipeline using joblib so that the exact preprocessing and prediction steps are preserved for future inference.